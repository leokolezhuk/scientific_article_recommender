{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "Load transformed data from a parquet file generated by the ETL."}, {"metadata": {}, "cell_type": "code", "source": "import ibmos2spark\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_1 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-f260c7dd-ea22-450d-ac4f-411d7bbe11a8',\n    'IBM_API_KEY_ID': 'fN2y36SJQsBNLN7BhTsrFNjuEIaYuqSsJn5059Il-meW',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'advanceddatasciencecapstone-donotdelete-pr-eegj6wm2sruxqi',\n    'FILE': 'arxiv_data_transformed.parquet'\n}\n\nconfiguration_name = 'os_e245254457804a5abc5f0819c8c11f69_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials_1, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n\n\ndf_arxiv_data = spark.read.parquet(cos.url('arxiv_data_transformed.parquet', 'advanceddatasciencecapstone-donotdelete-pr-eegj6wm2sruxqi'))", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200830144900-0002\nKERNEL_ID = d76277a0-2720-4f07-a887-d52d83edb1e0\n", "name": "stdout"}, {"output_type": "error", "ename": "ValueError", "evalue": "Invalid input: credentials. endpoint is required!", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-9587c48261bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mconfiguration_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'os_e245254457804a5abc5f0819c8c11f69_configs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibmos2spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCloudObjectStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bluemix_cos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/ibmos2spark/osconfig.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sparkcontext, credentials, configuration_name, cos_type, auth_method, bucket_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m         '''\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# check if all required values are availble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/ibmos2spark/osconfig.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, credentials, cos_type, auth_method)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_key_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid input: credentials. {} is required!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mValueError\u001b[0m: Invalid input: credentials. endpoint is required!"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "Define ranking function to rank articles against a given set of keywords. Higher rank indicates higher similarity."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.linalg import SparseVector, DenseVector\nfrom pyspark.sql import types as T\nfrom pyspark.sql.functions import udf\n\ndef rank(doc, keywords):\n    doc_keys = DenseVector(doc.indices)\n    doc_keys = list([int(x) for x in doc_keys])\n    \n    doc_values = DenseVector(doc.values)\n    doc_values = list([float(x) for x in doc_values])\n    \n    rank = 0.0\n    for keyword in keywords:\n        keyword_rank = 0.0\n        if keyword in doc_keys:\n            keyword_index = doc_keys.index(keyword)\n            keyword_rank = doc_values[keyword_index]\n        rank += keyword_rank\n        \n    return rank\n\ndef sparse_to_array(v):\n    v = DenseVector(v)\n    new_array = list([float(x) for x in v])\n    return new_array\n\n\nudf_rank = udf(rank, T.FloatType())", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Define function to get 5 be"}, {"metadata": {}, "cell_type": "code", "source": "import pyspark.sql.functions as SF\nfrom pyspark.sql.functions import desc\nfrom operator import itemgetter\n\ndef get_best_matches(article_name):\n    my_entry = df_arxiv_data.where(df_arxiv_data.title == article_name)\n    my_keywords = my_entry.select(\"title_vec\").collect()[0][0].indices.tolist()\n    my_keywords_importance = my_entry.select(\"title_vec\").collect()[0][0].values.tolist()\n    my_keywords_and_importances = list(zip(my_keywords, my_keywords_importance))\n\n    my_keywords_and_importances.sort(key=itemgetter(1), reverse=True)\n    my_most_important_keys = [d[0] for d in my_keywords_and_importances[:5]]\n\n    df_arxiv_data_ranked = df_arxiv_data.withColumn('rank', udf_rank(df_arxiv_data['title_vec'], SF.array([SF.lit(x) for x in my_most_important_keys])))\n    best_matches = df_arxiv_data_ranked.orderBy(desc(\"rank\")).take(5)\n    \n    return best_matches", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "best_matches = get_best_matches('Dynamic Routing Between Capsules')", "execution_count": 6, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'df_arxiv_data' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-6-cc253254dcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dynamic Routing Between Capsules'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m<ipython-input-5-520f3d8fb9a3>\u001b[0m in \u001b[0;36mget_best_matches\u001b[0;34m(article_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_best_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmy_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_arxiv_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_arxiv_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marticle_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmy_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_entry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title_vec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmy_keywords_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_entry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title_vec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'df_arxiv_data' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}